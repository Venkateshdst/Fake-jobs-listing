import re

import string
import numpy as np
import pandas as pd
import random
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.base import TransformerMixin
from sklearn.metrics import accuracy_score,plot_confusion_matrix,classification_report,confusion_matrix
from wordcloud import wordcloud
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from spacy.lang.en import English
df = pd.read_csv("fake_job_postings.csv")
print(df)
df.head()
df.shape
df.isnull().sum()
columns = ['job_id','telecommuting','has_company_logo','has_questions','salary_range','employment_type']
for colu in columns:
    del df[colu]
df.head()
df.fillna('',inplace=True)
plt.figure(figsize=(15,5))
sns.countplot(y='fraudulent', data=df)
plt.show()
df.groupby('fraudulent')['fraudulent'].count()
exp =dict(df.required_experience.value_counts())
del exp['']
exp
plt.figure(figsize=(10,5))
sns.set_theme(style ='whitegrid')
plt.bar(exp.keys(), exp.values())
plt.title('no of jobs with experience', size =10)
plt.xlabel('experience', size =10)
plt.ylabel('no of jobs', size =10)
plt.xticks(rotation=30)
plt.show()
def split(location):
    l = location.split('.')
    return l[0]
df['country'] = df.location.apply(split)
df.head()	
countr=dict(df.country.value_counts()[:14])
del countr['']
countr
plt.figure(figsize=(8,6))
plt.title('country-wise job posting', size=20)
plt.bar(countr.keys(), countr.values())
plt.ylabel('no of jobs', size =10)
plt.xlabel('countries', size =20)
edu = dict(df.required_education.value_counts()[:7])
del edu['']
edu
1
plt.figure(figsize =(15,6))
2
plt.title('job posting education level', size=20)
3
plt.bar(edu.keys(), edu.values())
4
plt.ylabel("jobs posting", size= 10)
5
plt.xlabel("education level", size=10)
6
plt.show()
print(df[df.fraudulent==0].title.value_counts()[:10])
print(df[df.fraudulent==1].title.value_counts()[:10])
df['text']=df['title']+' '+df['company_profile']+''+df['description']+''+df['requirements']+''+df['benefits']
del df['title']
del df['location']
del df['department']
del df['company_profile']
del df['description']
del df['requirements']
del df['benefits']
del df['required_experience']
del df['required_education']
del df['industry']
del df['function']
del df['country']	
df.head()
fraudjobs_text = df[df.fraudulent==1].text
realjobs_text = df[df.fraudulent==0].text
STOPWORDS = spacy.lang.en.stop_words.STOP_WORDS
plt.figure(figsize = (16,14))
wc = WordCloud(min_font_size = 3, max_words = 3000 , width = 1600, height= 800, stopwords = STOPWORDS).generate(str(" ".join(fraudjobs_text)))
plt.imshow(wc, interpolation = 'bilinear')  
STOPWORDS = spacy.lang.en.stop_words.STOP_WORDS
plt.figure(figsize = (16,14))
wc = WordCloud(min_font_size = 3, max_words = 3000 , width = 1600, height= 800, stopwords = STOPWORDS).generate(str(" ".join(realjobs_text)))
plt.imshow(wc,interpolation = 'bilinear')
!pip install spacy && python -m spacy download en
#CLEANING AND PREPROCESSING
#CREATING OUR LIST OF PUNCTUATIONS MARKS
punctuations = string.punctuation
#CREATING OUR LIST OF STOPWORDS
nlp = spacy.load("en_core_web_sm")
stop_words = spacy.lang.en.stop_words.STOP_WORDS
# LOAD ENGLISH TOKENIZER, TAGGER, PARSER, NER AND WORD VECTORS
PARSER = English()
#creating tokenizer function
def spacy_tokenizer(sentence):
    #creating our token object,which is used to create documents with linguistic annotations.
    mytokens = parser(sentence)
    #lemmaitazing each token and coverting each token into lowercase
    mytokens = [word.lemma_.lower().strip() if word.lemma_ !-".PRON-" else word.lower_for word in mytokens]
    #REMOVING STOP WORDS
    mytokens = [word for word in my tokens if word not in stop_words and word not in punctuations]
    #return a preprocessed list of tokens
    return mytokens
#custom transformer using spacy
class predictors(TranformerMixin):
    def transform(self, X, **tranform_params):
        #cleaning text
        return [clean_text(text) for text in X]
    def fit(self, X, y=None, **fit_params):
        return self
    def get_params(self, deep=True):
        return{}
#BASIC FUNCTION TO CLEAN THE TEXT
def clean_text(text):
    #REMOVING SPACES AND CONVERTING TEXT INTO LOWERCASE
    return text.strip().lower()	
df['text']= df['text'].apply[clean_text]
cv = TfidVectorizer(max_features=100)
x = cv.fit_transform(df['text'])
df1 =pd.DataFrameFrame(x,toarray(), columns=cv.get_feature_names())
df.drop(['text'], axis=1, inplace=True)
main_df = pd.conact([df1,df], axis=1)
main_df.head()
y = main_df.iloc[:,-1]
x = main_df.iloc[:,:-1]
x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.3)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)
from skelearn.ensemble import RandomForestClassfier
rfc = RandomForestClassifier(n_jobs=3,oob_score=True, n_estimatiors=100,criterion='entropy')
model=rfc.fit(x_train,y_train)
print(x_test)
pred = rfc.predict(x_test)
score = accuracy_score(y_test, pred)
score
print("Classification Rport \n")
print(classification_report(y_test,pred))
print("confusion Matrix\n")
print("confusion_matrix(y_test,pred)")    	

